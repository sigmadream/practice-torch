{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pytorch_lightning as pl\n",
    "import transformers\n",
    "import pandas as pd\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torchmetrics.functional import accuracy\n",
    "from transformers import BertModel, BertConfig\n",
    "from transformers import AutoModel, BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pub_health_train = pd.read_csv(\"./PUBHEALTH/train.tsv\", sep='\\t')\n",
    "pub_health_test = pd.read_csv(\"./PUBHEALTH/test.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>main_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"Hillary Clinton is in the political crosshair...</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>While the financial costs of screening mammogr...</td>\n",
       "      <td>mixture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The news release quotes lead researcher Robert...</td>\n",
       "      <td>mixture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The story does discuss costs, but the framing ...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Although the story didn’t cite the cost of ap...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           main_text    label\n",
       "0  \"Hillary Clinton is in the political crosshair...    false\n",
       "1  While the financial costs of screening mammogr...  mixture\n",
       "2  The news release quotes lead researcher Robert...  mixture\n",
       "3  The story does discuss costs, but the framing ...     true\n",
       "4  \"Although the story didn’t cite the cost of ap...     true"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pub_health_train = pub_health_train[pub_health_train['label'] != 'snopes']\n",
    "pub_health_train = pub_health_train[['main_text','label']]\n",
    "pub_health_train = pub_health_train.dropna(subset=['main_text', 'label'])\n",
    "pub_health_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pub_health_test = pub_health_test[['main_text','label']]\n",
    "pub_health_test = pub_health_test.dropna(subset=['main_text', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pub_health_train['label'] = pub_health_train['label'].map({\"true\":0, \"false\":1, \"unproven\":2, \"mixture\":3})\n",
    "pub_health_test['label'] = pub_health_test['label'].map({\"true\":0, \"false\":1, \"unproven\":2, \"mixture\":3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HealthClaimClassifier(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, max_seq_len=512, batch_size=64, learning_rate = 0.001):\n",
    "        super().__init__()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.pretrain_model  = AutoModel.from_pretrained('bert-base-uncased', return_dict=False)\n",
    "        self.pretrain_model.eval()\n",
    "        for param in self.pretrain_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.new_layers = nn.Sequential(\n",
    "            nn.Linear(768, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512,4),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def prepare_data(self):\n",
    "        tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased', return_dict=False)\n",
    "\n",
    "        tokens_train = tokenizer.batch_encode_plus(\n",
    "            pub_health_train[\"main_text\"].tolist(),\n",
    "            max_length = self.max_seq_len,\n",
    "            pad_to_max_length=True,\n",
    "            truncation=True,\n",
    "            return_token_type_ids=False\n",
    "        )\n",
    "\n",
    "        tokens_test = tokenizer.batch_encode_plus(\n",
    "            pub_health_test[\"main_text\"].tolist(),\n",
    "            max_length = self.max_seq_len,\n",
    "            pad_to_max_length=True,\n",
    "            truncation=True,\n",
    "            return_token_type_ids=False\n",
    "        )\n",
    "\n",
    "        self.train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "        self.train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "        self.train_y = torch.tensor(pub_health_train[\"label\"].tolist())\n",
    "\n",
    "        self.test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "        self.test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "        self.test_y = torch.tensor(pub_health_test[\"label\"].tolist())\n",
    "\n",
    "    def forward(self, encode_id, mask):\n",
    "        _, output= self.pretrain_model(encode_id, attention_mask=mask)\n",
    "        output = self.new_layers(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_dataset = TensorDataset(self.train_seq, self.train_mask, self.train_y)\n",
    "        self.train_dataloader_obj = DataLoader(train_dataset, batch_size=self.batch_size)\n",
    "        return self.train_dataloader_obj\n",
    "\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        test_dataset = TensorDataset(self.test_seq, self.test_mask, self.test_y)\n",
    "        self.test_dataloader_obj = DataLoader(test_dataset, batch_size=self.batch_size)\n",
    "        return self.test_dataloader_obj\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        encode_id, mask, targets = batch\n",
    "        outputs = self(encode_id, mask) \n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        train_accuracy = accuracy(preds, targets)\n",
    "        loss = self.loss(outputs, targets)\n",
    "        self.log('train_accuracy', train_accuracy, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True)\n",
    "        return {\"loss\":loss, 'train_accuracy': train_accuracy}\n",
    "\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        encode_id, mask, targets = batch\n",
    "        outputs = self.forward(encode_id, mask)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        test_accuracy = accuracy(preds, targets)\n",
    "        loss = self.loss(outputs, targets)\n",
    "        return {\"test_loss\":loss, \"test_accuracy\":test_accuracy}\n",
    "        \n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        test_outs = []\n",
    "        for test_out in outputs:\n",
    "            out = test_out['test_accuracy']\n",
    "            test_outs.append(out)\n",
    "        total_test_accuracy = torch.stack(test_outs).mean()\n",
    "        self.log('total_test_accuracy', total_test_accuracy, on_step=False, on_epoch=True)\n",
    "        return total_test_accuracy\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        params = self.parameters()\n",
    "        optimizer = optim.Adam(params=params, lr = self.learning_rate)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type             | Params\n",
      "----------------------------------------------------\n",
      "0 | loss           | CrossEntropyLoss | 0     \n",
      "1 | pretrain_model | BertModel        | 109 M \n",
      "2 | new_layers     | Sequential       | 395 K \n",
      "----------------------------------------------------\n",
      "395 K     Trainable params\n",
      "109 M     Non-trainable params\n",
      "109 M     Total params\n",
      "439.512   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70cb4178d28e4f47a8c09f76e4d52529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=1` reached.\n"
     ]
    }
   ],
   "source": [
    "model = HealthClaimClassifier()\n",
    "\n",
    "trainer = pl.Trainer(fast_dev_run=True, gpus=1)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type             | Params\n",
      "----------------------------------------------------\n",
      "0 | loss           | CrossEntropyLoss | 0     \n",
      "1 | pretrain_model | BertModel        | 109 M \n",
      "2 | new_layers     | Sequential       | 395 K \n",
      "----------------------------------------------------\n",
      "395 K     Trainable params\n",
      "109 M     Non-trainable params\n",
      "109 M     Total params\n",
      "439.512   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59c3ef63991c4aec9ec09ca7329ccc48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "model = HealthClaimClassifier()\n",
    "trainer = pl.Trainer(max_epochs=10, gpus=-1)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sd/works/practice-torch/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:134: UserWarning: `.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.test(ckpt_path='best')` to use the best model or `.test(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n",
      "  rank_zero_warn(\n",
      "Restoring states from the checkpoint path at /home/sd/works/practice-torch/pl-notes/lightning_logs/version_1/checkpoints/epoch=9-step=1540.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from checkpoint at /home/sd/works/practice-torch/pl-notes/lightning_logs/version_1/checkpoints/epoch=9-step=1540.ckpt\n",
      "/home/sd/works/practice-torch/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac52461b338f42daa9b1b1fc77c11003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "   total_test_accuracy      0.5993106961250305\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'total_test_accuracy': 0.5993106961250305}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ffa44ed55e81a95cb95a71332f3aebc72df64d87824a1002d9fb2a7161c5cc64"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
